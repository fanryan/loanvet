{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa26483",
   "metadata": {},
   "source": [
    "# 02 | Advanced Models\n",
    "\n",
    "**Objective:**  \n",
    "Train advanced ensemble models—Random Forest, XGBoost, LightGBM on the fully engineered dataset to evaluate performance gains over the logistic regression baseline.\n",
    "\n",
    "**Approach:**  \n",
    "- Use the fully engineered feature set to leverage interactions, binning, and transformations.\n",
    "- Train Random Forest, XGBoost, and LightGBM models using 5-fold stratified cross-validation to ensure robust evaluation.\n",
    "- Assess performance using ROC-AUC, PR-AUC, F1 score, and confusion matrix.\n",
    "- Interpret results through feature importance rankings from each model to understand key predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a9bdb09",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1403b7",
   "metadata": {},
   "source": [
    "## Load Engineered Dataset  \n",
    "We’ll pull from our `credit_risk_engineered` table in the SQLite DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b3dd4f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalDelinquencies_log</th>\n",
       "      <th>HighUtilizationFlag</th>\n",
       "      <th>IncomePerCreditLine</th>\n",
       "      <th>AgeGroup_MidAge</th>\n",
       "      <th>AgeGroup_Senior</th>\n",
       "      <th>DependentsGroup_Small</th>\n",
       "      <th>DependentsGroup_Large</th>\n",
       "      <th>Util_x_Late</th>\n",
       "      <th>IncomePerDependent</th>\n",
       "      <th>CreditLines_x_Delinquencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0</td>\n",
       "      <td>651.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3040.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350539</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1</td>\n",
       "      <td>6250.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  age  \\\n",
       "0                 1                              0.766127   45   \n",
       "1                 0                              0.957151   40   \n",
       "2                 0                              0.658180   38   \n",
       "3                 0                              0.233810   30   \n",
       "4                 0                              0.907239   49   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                     2   0.802982         9120.0   \n",
       "1                                     0   0.121876         2600.0   \n",
       "2                                     1   0.085113         3042.0   \n",
       "3                                     0   0.036050         3300.0   \n",
       "4                                     1   0.024926        50000.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                               13                        0   \n",
       "1                                4                        0   \n",
       "2                                2                        1   \n",
       "3                                5                        0   \n",
       "4                                7                        0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  ...  \\\n",
       "0                             6                                     0  ...   \n",
       "1                             0                                     0  ...   \n",
       "2                             0                                     0  ...   \n",
       "3                             0                                     0  ...   \n",
       "4                             1                                     0  ...   \n",
       "\n",
       "   TotalDelinquencies_log  HighUtilizationFlag  IncomePerCreditLine  \\\n",
       "0                1.098612                    0           651.428571   \n",
       "1                0.000000                    1           520.000000   \n",
       "2                1.098612                    0          1014.000000   \n",
       "3                0.000000                    0           550.000000   \n",
       "4                0.693147                    1          6250.000000   \n",
       "\n",
       "   AgeGroup_MidAge  AgeGroup_Senior  DependentsGroup_Small  \\\n",
       "0                1                0                      1   \n",
       "1                1                0                      1   \n",
       "2                1                0                      0   \n",
       "3                1                0                      0   \n",
       "4                1                0                      0   \n",
       "\n",
       "   DependentsGroup_Large  Util_x_Late  IncomePerDependent  \\\n",
       "0                      0     0.000000              3040.0   \n",
       "1                      0     0.000000              1300.0   \n",
       "2                      0     0.350539              3042.0   \n",
       "3                      0     0.000000              3300.0   \n",
       "4                      0     0.000000             50000.0   \n",
       "\n",
       "   CreditLines_x_Delinquencies  \n",
       "0                           26  \n",
       "1                            0  \n",
       "2                            4  \n",
       "3                            0  \n",
       "4                            7  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"../data/loanvet.db\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM credit_risk_engineered\", conn)\n",
    "conn.close()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0a13f",
   "metadata": {},
   "source": [
    "## Drop Redundant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5bcb56c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"RevolvingUtilizationOfUnsecuredLines\",\n",
    "    \"DebtRatio\",\n",
    "    \"NumberOfTime30-59DaysPastDueNotWorse\",\n",
    "    \"NumberOfTimes90DaysLate\",\n",
    "    \"NumberOfTime60-89DaysPastDueNotWorse\",\n",
    "    'NumberOfTime30-59DaysPastDueNotWorse_log',\n",
    "    'NumberOfTimes90DaysLate_log',\n",
    "    'NumberOfTime60-89DaysPastDueNotWorse_log',\n",
    "    \"TotalDelinquencies\",\n",
    "    \"MonthlyIncome\"\n",
    "]\n",
    "df = df.drop(columns=drop_cols, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f32d45",
   "metadata": {},
   "source": [
    "## Data Preprocessing Notes\n",
    "To reduce multicollinearity and avoid redundancy, we dropped raw features that were either highly skewed or already represented by transformed versions:\n",
    "\n",
    "- Original delinquency variables (e.g., `NumberOfTimes90DaysLate`) were replaced by their log-transformed versions or aggregated into `TotalDelinquencies_log`.\n",
    "- Skewed continuous variables like `RevolvingUtilizationOfUnsecuredLines` and `MonthlyIncome` were dropped in favor of their log-transformed counterparts.\n",
    "- The original `TotalDelinquencies` was also removed since the log-transformed version was used for modeling.\n",
    "\n",
    "This step ensures the model focuses on cleaner, more interpretable signals while minimizing the risk of redundant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54831c2",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "Stratify on the target to preserve imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62c2f9cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"SeriousDlqin2yrs\"])\n",
    "y = df[\"SeriousDlqin2yrs\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c7a30",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "Train a Random Forest model with balanced class weights to handle class imbalance. The model uses 100 trees (n_estimators=100) and a fixed random state for reproducibility. Predicted probabilities for the positive class are generated for performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "988c4fdc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test Split ROC-AUC: 0.8362\n",
      "Train/Test Split PR-AUC: 0.3549\n",
      "Train/Test Split F1 Score: 0.2635\n",
      "Train/Test Split Confusion Matrix:\n",
      "[[27599   278]\n",
      " [ 1656   346]]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "rf_y_pred_prob = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "rf_y_pred = (rf_y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, rf_y_pred_prob)\n",
    "precision, recall, _ = precision_recall_curve(y_test, rf_y_pred_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "f1 = f1_score(y_test, rf_y_pred)\n",
    "\n",
    "print(f\"Train/Test Split ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Train/Test Split PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"Train/Test Split F1 Score: {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, rf_y_pred)\n",
    "print(\"Train/Test Split Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a544cda",
   "metadata": {},
   "source": [
    "## XGBoost Classifier\n",
    "Train an XGBoost model using 100 trees with a maximum depth of 3 and a learning rate of 0.1. The model is optimised with the logistic loss (logloss) metric. Predicted probabilities for the positive class are generated for performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1598aa4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test Split ROC-AUC: 0.8637\n",
      "Train/Test Split PR-AUC: 0.4029\n",
      "Train/Test Split F1 Score: 0.2826\n",
      "Train/Test Split Confusion Matrix:\n",
      "[[27618   259]\n",
      " [ 1630   372]]\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_y_pred_prob = xgb.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, xgb_y_pred_prob)\n",
    "precision, recall, _ = precision_recall_curve(y_test, xgb_y_pred_prob)\n",
    "\n",
    "xgb_y_pred = (xgb_y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, xgb_y_pred_prob)\n",
    "precision, recall, _ = precision_recall_curve(y_test, xgb_y_pred_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "f1 = f1_score(y_test, xgb_y_pred)\n",
    "\n",
    "print(f\"Train/Test Split ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Train/Test Split PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"Train/Test Split F1 Score: {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, xgb_y_pred)\n",
    "print(\"Train/Test Split Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff721a2",
   "metadata": {},
   "source": [
    "## LightGBM Classifier\n",
    "Train a LightGBM model using 100 trees with a maximum depth of 3 and a learning rate of 0.1. The model is optimised with the binary logistic loss function. Predicted probabilities for the positive class are generated for performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8fe1f52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8007, number of negative: 111505\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1814\n",
      "[LightGBM] [Info] Number of data points in the train set: 119512, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Train/Test Split ROC-AUC: 0.8633\n",
      "Train/Test Split PR-AUC: 0.3986\n",
      "Train/Test Split F1 Score: 0.3452\n",
      "Train/Test Split Confusion Matrix:\n",
      "[[22462  5415]\n",
      " [  455  1547]]\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42, class_weight='balanced')\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_y_pred_prob = lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, lgbm_y_pred_prob)\n",
    "precision, recall, _ = precision_recall_curve(y_test, lgbm_y_pred_prob)\n",
    "\n",
    "lgbm_y_pred = (lgbm_y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, lgbm_y_pred_prob)\n",
    "precision, recall, _ = precision_recall_curve(y_test, lgbm_y_pred_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "f1 = f1_score(y_test, lgbm_y_pred)\n",
    "\n",
    "print(f\"Train/Test Split ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Train/Test Split PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"Train/Test Split F1 Score: {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, lgbm_y_pred)\n",
    "print(\"Train/Test Split Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f0837",
   "metadata": {},
   "source": [
    "### Preliminary Train/Test Split Evaluation\n",
    "\n",
    "Before performing more robust cross-validation, we conduct an initial evaluation using a single train/test split.\n",
    "\n",
    "- This quick check ensures our pipeline works end-to-end without errors.\n",
    "- However, metrics from a single split can be unstable and heavily dependent on how the data is divided.\n",
    "- Therefore, these results serve as a sanity check rather than a final performance measure.\n",
    "\n",
    "For reliable and generalizable model evaluation, we proceed with stratified 5-fold cross-validation, which averages performance across multiple splits and reduces variance due to random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf43075",
   "metadata": {},
   "source": [
    "## Performance Metrics  \n",
    "\n",
    "Evaluate model using key classification metrics that reflect imbalanced data performance:\n",
    "\n",
    "ROC-AUC: Measures overall ranking ability between classes.\n",
    "\n",
    "PR-AUC: Focuses on precision and recall trade-off, important for imbalanced datasets.\n",
    "\n",
    "F1 Score: Harmonic mean of precision and recall at 0.5 threshold.\n",
    "\n",
    "Confusion Matrix: Summarizes true positives, false positives, true negatives, and false negatives across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b638e677",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8007, number of negative: 111505\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1814\n",
      "[LightGBM] [Info] Number of data points in the train set: 119512, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066997 -> initscore=-2.633753\n",
      "[LightGBM] [Info] Start training from score -2.633753\n",
      "[LightGBM] [Info] Number of positive: 8007, number of negative: 111506\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1813\n",
      "[LightGBM] [Info] Number of data points in the train set: 119513, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066997 -> initscore=-2.633762\n",
      "[LightGBM] [Info] Start training from score -2.633762\n",
      "[LightGBM] [Info] Number of positive: 8007, number of negative: 111506\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1813\n",
      "[LightGBM] [Info] Number of data points in the train set: 119513, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066997 -> initscore=-2.633762\n",
      "[LightGBM] [Info] Start training from score -2.633762\n",
      "[LightGBM] [Info] Number of positive: 8007, number of negative: 111506\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1816\n",
      "[LightGBM] [Info] Number of data points in the train set: 119513, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066997 -> initscore=-2.633762\n",
      "[LightGBM] [Info] Start training from score -2.633762\n",
      "[LightGBM] [Info] Number of positive: 8008, number of negative: 111505\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1815\n",
      "[LightGBM] [Info] Number of data points in the train set: 119513, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.067005 -> initscore=-2.633628\n",
      "[LightGBM] [Info] Start training from score -2.633628\n",
      "Random Forest ROC-AUC: 0.8313 ± 0.0048\n",
      "Random Forest PR-AUC: 0.3446 ± 0.0115\n",
      "Random Forest F1: 0.2466 ± 0.0053\n",
      "Random Forest Summed Confusion Matrix:\n",
      "[[138052   1330]\n",
      " [  8414   1595]]\n",
      "\n",
      "XGBoost ROC-AUC: 0.8647 ± 0.0037\n",
      "XGBoost PR-AUC: 0.3988 ± 0.0109\n",
      "XGBoost F1: 0.2888 ± 0.0063\n",
      "XGBoost Summed Confusion Matrix:\n",
      "[[138132   1250]\n",
      " [  8108   1901]]\n",
      "\n",
      "LightGBM ROC-AUC: 0.8636 ± 0.0035\n",
      "LightGBM PR-AUC: 0.3965 ± 0.0112\n",
      "LightGBM F1: 0.2851 ± 0.0087\n",
      "LightGBM Summed Confusion Matrix:\n",
      "[[138128   1254]\n",
      " [  8136   1873]]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rf_roc_auc_scores, rf_pr_auc_scores, rf_f1_scores = [], [], []\n",
    "xgb_roc_auc_scores, xgb_pr_auc_scores, xgb_f1_scores = [], [], []\n",
    "lgb_roc_auc_scores, lgb_pr_auc_scores, lgb_f1_scores = [], [], []\n",
    "\n",
    "rf_cm_sum = np.array([[0, 0], [0, 0]])\n",
    "xgb_cm_sum = np.array([[0, 0], [0, 0]])\n",
    "lgb_cm_sum = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "def calc_metrics(y_true, y_scores, threshold=0.5):\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return roc_auc, pr_auc, f1, cm\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred_prob = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # XGBoost\n",
    "    xgb = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "                        eval_metric='logloss', random_state=42)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    xgb_pred_prob = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # LightGBM\n",
    "    lgb = LGBMClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                        random_state=42)\n",
    "    lgb.fit(X_train, y_train)\n",
    "    lgb_pred_prob = lgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    rf_roc, rf_pr, rf_f1, rf_cm = calc_metrics(y_test, rf_pred_prob)\n",
    "    xgb_roc, xgb_pr, xgb_f1, xgb_cm = calc_metrics(y_test, xgb_pred_prob)\n",
    "    lgb_roc, lgb_pr, lgb_f1, lgb_cm = calc_metrics(y_test, lgb_pred_prob)\n",
    "\n",
    "    rf_roc_auc_scores.append(rf_roc)\n",
    "    rf_pr_auc_scores.append(rf_pr)\n",
    "    rf_f1_scores.append(rf_f1)\n",
    "    rf_cm_sum += rf_cm\n",
    "\n",
    "    xgb_roc_auc_scores.append(xgb_roc)\n",
    "    xgb_pr_auc_scores.append(xgb_pr)\n",
    "    xgb_f1_scores.append(xgb_f1)\n",
    "    xgb_cm_sum += xgb_cm\n",
    "\n",
    "    lgb_roc_auc_scores.append(lgb_roc)\n",
    "    lgb_pr_auc_scores.append(lgb_pr)\n",
    "    lgb_f1_scores.append(lgb_f1)\n",
    "    lgb_cm_sum += lgb_cm\n",
    "\n",
    "    if fold_idx == skf.get_n_splits(X, y) - 1:\n",
    "        last_fold = {\n",
    "            'y_test': y_test,\n",
    "            'rf_prob': rf_pred_prob,\n",
    "            'xgb_prob': xgb_pred_prob,\n",
    "            'lgb_prob': lgb_pred_prob\n",
    "        }\n",
    "\n",
    "print(f\"Random Forest ROC-AUC: {np.mean(rf_roc_auc_scores):.4f} ± {np.std(rf_roc_auc_scores):.4f}\")\n",
    "print(f\"Random Forest PR-AUC: {np.mean(rf_pr_auc_scores):.4f} ± {np.std(rf_pr_auc_scores):.4f}\")\n",
    "print(f\"Random Forest F1: {np.mean(rf_f1_scores):.4f} ± {np.std(rf_f1_scores):.4f}\")\n",
    "print(f\"Random Forest Summed Confusion Matrix:\\n{rf_cm_sum}\\n\")\n",
    "\n",
    "print(f\"XGBoost ROC-AUC: {np.mean(xgb_roc_auc_scores):.4f} ± {np.std(xgb_roc_auc_scores):.4f}\")\n",
    "print(f\"XGBoost PR-AUC: {np.mean(xgb_pr_auc_scores):.4f} ± {np.std(xgb_pr_auc_scores):.4f}\")\n",
    "print(f\"XGBoost F1: {np.mean(xgb_f1_scores):.4f} ± {np.std(xgb_f1_scores):.4f}\")\n",
    "print(f\"XGBoost Summed Confusion Matrix:\\n{xgb_cm_sum}\\n\")\n",
    "\n",
    "print(f\"LightGBM ROC-AUC: {np.mean(lgb_roc_auc_scores):.4f} ± {np.std(lgb_roc_auc_scores):.4f}\")\n",
    "print(f\"LightGBM PR-AUC: {np.mean(lgb_pr_auc_scores):.4f} ± {np.std(lgb_pr_auc_scores):.4f}\")\n",
    "print(f\"LightGBM F1: {np.mean(lgb_f1_scores):.4f} ± {np.std(lgb_f1_scores):.4f}\")\n",
    "print(f\"LightGBM Summed Confusion Matrix:\\n{lgb_cm_sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57350cc7",
   "metadata": {},
   "source": [
    "## ROC-AUC and Precision-Recall Curves\n",
    "\n",
    "To visually compare model performance, we plot the ROC and Precision-Recall curves for the final fold of cross-validation. These plots provide insight into each model's ability to separate the positive and negative classes, especially in the context of class imbalance.\n",
    "\n",
    "- **ROC Curve**: Shows the trade-off between true positive rate and false positive rate. A higher curve indicates better discriminative ability.\n",
    "- **Precision-Recall Curve**: More informative for imbalanced datasets, highlighting how well the model maintains precision across recall levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a985a85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rf_pred_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m y_test \u001b[38;5;241m=\u001b[39m last_fold[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, prob \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mlast_fold\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrf_pred_prob\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m),\n\u001b[1;32m      4\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m, last_fold[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb_pred_prob\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      5\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM\u001b[39m\u001b[38;5;124m'\u001b[39m, last_fold[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb_pred_prob\u001b[39m\u001b[38;5;124m'\u001b[39m])]:\n\u001b[1;32m      6\u001b[0m     fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_test, prob)\n\u001b[1;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(fpr, tpr, label\u001b[38;5;241m=\u001b[39mlabel)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rf_pred_prob'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test = last_fold['y_test']\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label, prob in [('Random Forest', last_fold['rf_prob']),\n",
    "                    ('XGBoost', last_fold['xgb_prob']),\n",
    "                    ('LightGBM', last_fold['lgb_prob'])]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
    "    plt.plot(fpr, tpr, label=label)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (Last Fold)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label, prob in [('Random Forest', last_fold['rf_prob']),\n",
    "                    ('XGBoost', last_fold['xgb_prob']),\n",
    "                    ('LightGBM', last_fold['lgb_prob'])]:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, prob)\n",
    "    plt.plot(recall, precision, label=label)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve (Last Fold)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fd52b",
   "metadata": {},
   "source": [
    "### Model Performance Interpretation\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "- **ROC-AUC (0.8313 ± 0.0048)**  \n",
    "  The model has good discrimination ability between defaulters and non-defaulters, but is the weakest among the three models here.\n",
    "\n",
    "- **PR-AUC (0.3446 ± 0.0115)**  \n",
    "  Precision-recall performance is moderate, reflecting the challenge of predicting the minority (positive) class in this imbalanced dataset.\n",
    "\n",
    "- **F1 Score (0.2466 ± 0.0053)**  \n",
    "  The harmonic mean of precision and recall is relatively low, indicating a conservative model with more false negatives or false positives.\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "- **ROC-AUC (0.8647 ± 0.0037)**  \n",
    "Strong discrimination ability, best among the three models.\n",
    "\n",
    "- **PR-AUC (0.3988 ± 0.0109)**  \n",
    "Better precision-recall performance, indicating improved detection of the minority class.\n",
    "\n",
    "- **F1 Score (0.2888 ± 0.0063)**  \n",
    "Better balance between precision and recall compared to Random Forest.\n",
    "\n",
    "#### LightGBM\n",
    "\n",
    "- **ROC-AUC (0.8636 ± 0.0035)**  \n",
    "Comparable discrimination to XGBoost, slightly behind by a very small margin.\n",
    "\n",
    "- **PR-AUC (0.3965 ± 0.0112)**  \n",
    "Similar precision-recall performance as XGBoost, indicating competitive detection of the minority class.\n",
    "\n",
    "- **F1 Score (0.2851 ± 0.0087)**  \n",
    "Balanced precision and recall close to XGBoost, with slightly lower F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab32fb3",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- All three models show **strong ROC-AUC** (~0.83–0.86), meaning good overall ranking ability.\n",
    "- **XGBoost and LightGBM outperform Random Forest** in precision-recall and F1 metrics, indicating better handling of class imbalance.\n",
    "- Despite strong metrics, **false negatives remain substantial** (~8,100+)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
